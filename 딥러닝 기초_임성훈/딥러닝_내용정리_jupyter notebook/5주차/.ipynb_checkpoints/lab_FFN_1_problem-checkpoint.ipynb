{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Introduction to Deep Learning with Pytorch\n",
    "\n",
    "* 본 실습에서는 총 3개의 exercise가 있습니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "* 이번 실습에서는 [PyTorch](http://pytorch.org/)에 대해서 소개하겠니다. \n",
    "* numpy array 역시 tensor 이기때문에 Pytorch는 기본적으로 numpy와 매우 유사한 점이 많이 있습니다 (Pytorch의 디자인 철학이기도 합니다)\n",
    "* 또한 GPU를 활용하기 위해서 매우 편리한 방법을 제공합니다\n",
    "* 궁극적으로 network 만드는 것 부터 network training을 하기 위한 유용한 모듈을 모두 제공합니다. \n",
    "* 또한 Tensorflow에 비해서 Numpy/scipy와 더욱 유기적으로 작업이 가능합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "* Neural network (NN)을 활용하기 위해서 필요한 수학적 연산은 대부분 \n",
    "*tensors* 단위로 수행하는 선형 연산입니다\n",
    "* Tensor는 2차원 이상의 array이며 matrix, vector의 일반화된 개체입니다\n",
    " - vector 는 1-dimensional tensor\n",
    " - Matrix 는 2-dimensional tensor, \n",
    " - 3-dimensional tensor (예) RGB color images\n",
    "* 이와 같은 이유로 pytorch의 가장 기본적인 data structure는 tensors 입니다\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "그럼 첫 단계를 시작해보죠!\n",
    "\n",
    "가장 우선적으로 \n",
    "1. python에서 필요한 package를 불러와야합니다\n",
    "2. pytorch 불러오는 명령어는 다음과 같습니다. 거의 항상 첫줄에 아래 명령어를 사용한다고 생각하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "# First, import PyTorch\n",
    "import torch\n",
    "import numpy as np\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tensor\n",
    "`torch.tensor()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing tensors\n",
    "* 1D tensor를 index로 들어 있는 자료를 불러오면 scalar (0D tensor)로 return\n",
    "  * python scalar로 불러오기위해서 item() 활용\n",
    "* 2D tensor를 index로 들어 있는 자료를 불러오면 vector (1D tensor)로 return\n",
    "* 3D tensor를 index로 들어 있는 자료를 불러오면 matrix (2D tensor)로 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor([1., 2., 3.])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dtype=torch.data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "V_integer = torch.tensor([1.0, 2.0, 3.0], dtype=torch.int)\n",
    "print(V_integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random Gaussians $\\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4329, -0.8261, -0.3230, -0.2012, -0.7572],\n",
      "         [ 0.9558,  0.1524, -0.5267, -0.7010,  1.6860],\n",
      "         [-0.0171, -0.6956, -0.5280,  0.0354, -0.7516],\n",
      "         [ 1.0376,  0.3869, -1.2719,  1.0490,  0.8589]],\n",
      "\n",
      "        [[-0.5772, -0.4624,  0.3307,  0.1433,  1.5560],\n",
      "         [ 0.4705,  1.7702, -0.6031,  1.9666, -0.0286],\n",
      "         [-0.0192, -0.2696,  0.5941, -0.8741, -0.5719],\n",
      "         [-1.4852,  1.0875,  0.9100, -0.4243, -0.7704]],\n",
      "\n",
      "        [[-2.3253, -0.2038,  0.1531,  0.4596, -0.5290],\n",
      "         [-0.5955,  1.5410, -0.7018, -1.0801, -0.0078],\n",
      "         [-0.3605,  0.1488,  0.5377, -1.2628, -0.0755],\n",
      "         [-1.9379, -0.5651,  0.4143,  0.2677,  0.8293]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1254, -0.6844, -0.0869, -1.4988,  0.5784],\n",
      "        [-0.3819, -0.6912,  1.8819,  1.3687,  0.6280],\n",
      "        [ 0.9521,  0.3495, -1.2039,  1.4215,  0.4364],\n",
      "        [ 1.0408,  0.6057,  0.0282, -0.9339, -0.6888],\n",
      "        [-1.0372, -0.5033, -2.1676,  0.3176,  0.5270]])\n",
      "tensor([[ 0.6501,  0.8066,  1.6147,  1.3248,  1.2949,  0.6675,  0.4542,  1.9585],\n",
      "        [ 0.3938, -0.3442, -0.0936, -0.3033,  1.5791, -1.0839, -0.3498,  1.2810]])\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# Tensor의 크기가 잘 맞지 않으면 error 발생함\n",
    "# torch.cat([x_1, x_2])\n",
    "# print(x_1.size())\n",
    "# print(x_2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Tensors\n",
    "* Use the .view() method to reshape a tensor\n",
    "* This method receives heavy use, because many neural network components expect their inputs to have a certain shape\n",
    "* Often you will need to reshape before passing your data to the component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2894,  0.3778, -1.1799, -0.1093],\n",
      "         [-0.7973,  0.6412, -0.0043,  0.5819],\n",
      "         [ 0.8018, -0.3218, -0.1564,  0.5567]],\n",
      "\n",
      "        [[-1.1246,  0.1745,  0.1401, -0.4955],\n",
      "         [-1.1170,  0.5452,  0.1458,  0.5607],\n",
      "         [ 0.1798,  0.7112,  0.6867, -1.3928]]])\n",
      "tensor([[ 1.2894,  0.3778, -1.1799, -0.1093, -0.7973,  0.6412, -0.0043,  0.5819,\n",
      "          0.8018, -0.3218, -0.1564,  0.5567],\n",
      "        [-1.1246,  0.1745,  0.1401, -0.4955, -1.1170,  0.5452,  0.1458,  0.5607,\n",
      "          0.1798,  0.7112,  0.6867, -1.3928]])\n",
      "tensor([[ 1.2894,  0.3778, -1.1799, -0.1093, -0.7973,  0.6412, -0.0043,  0.5819,\n",
      "          0.8018, -0.3218, -0.1564,  0.5567],\n",
      "        [-1.1246,  0.1745,  0.1401, -0.4955, -1.1170,  0.5452,  0.1458,  0.5607,\n",
      "          0.1798,  0.7112,  0.6867, -1.3928]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "\n",
    "* 마무리 하면서, 간단하게 pytorch와 numpy간 자료변경하는 방법을 review 합니다\n",
    "* Numpy array에서 torch tensor로 자료변경은 `torch.from_numpy()` method를 사용합니다.\n",
    "* torch tensor를 numpy array로 변경은 `.numpy()` method를 사용합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27380266, 0.29089683, 0.24046682],\n",
       "       [0.63872439, 0.27899071, 0.12324684],\n",
       "       [0.90802378, 0.75098219, 0.84886351],\n",
       "       [0.19872754, 0.2278203 , 0.17771148]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27380266, 0.29089683, 0.24046682],\n",
       "       [0.63872439, 0.27899071, 0.12324684],\n",
       "       [0.90802378, 0.75098219, 0.84886351],\n",
       "       [0.19872754, 0.2278203 , 0.17771148]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위에서 수행한 모든 작업은 in-place 변환으로 (메모리를 새로 할당하지 않음) 두 개체는 memory 관점에서 같습니다. 즉, 하나를 변경하면 다른 하나도 변합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5476, 0.5818, 0.4809],\n",
       "        [1.2774, 0.5580, 0.2465],\n",
       "        [1.8160, 1.5020, 1.6977],\n",
       "        [0.3975, 0.4556, 0.3554]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54760533, 0.58179366, 0.48093363],\n",
       "       [1.27744879, 0.55798141, 0.24649368],\n",
       "       [1.81604756, 1.50196439, 1.69772703],\n",
       "       [0.39745508, 0.4556406 , 0.35542295]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "* Perceptron\n",
    " - Perceptron은 deep neural network에서 가장 기본적인 neuron을 본뜬 단위이며, 동작 방식은 input vector에 weight를 곱하고 더하여 (결국 innerproduct) activation function이라는 함수에 결과값을 출력하는 구조입니다. \n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "수식: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Vector inner product:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "* 예제로 activation fuction 하나를 만들어 보겠습니다.\n",
    "* 다음 activation function을 완성하세요\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, feature, weight tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # random 함수사용을 위한 seed 설정입니다\n",
    "\n",
    "# Data (feature)\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables\n",
    "weights = torch.randn_like(features)\n",
    "# bias term\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 줄별로 작업한 부분을 살펴보죠\n",
    "\n",
    " - `features = torch.randn((1, 5))` \n",
    "   - shape `(1, 5)` , one row and five columns, \n",
    "   - 정규분보 (평균 = 0, 표준편차 1)\n",
    "   - 왜 평균과 표준편차를 위와 같이 고정해도 되나요?\n",
    "\n",
    " - `weights = torch.randn_like(features)` \n",
    "   - `features`와 같은 shape로 randn을 불러옵니다 (편리하죠)\n",
    " -  `bias = torch.randn((1, 1))` \n",
    "   \n",
    "\n",
    "- PyTorch tensors 는 numpy arrray와 같이 기본적 연산이 가능합니다 (-,+,* 등) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 [2점]**: 위에서 정의한 `features`, `weights`, `bias`, `activation`을 활용하여 아래 수식을 완성하세요. \n",
    "  - [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) 함수 또는, *some_tensor*`.sum()` 사용\n",
    "  - 아래 $f(\\cdot)$ 함수는 위에서 정의한  `activation` 함수입니다, $x$는 `feature`, $w$는 `weights`, $b$는 `bias`입니다\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "`Output`:\n",
    "\n",
    "`tensor([[0.1595]])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6619]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "y = torch.sum(features * weights) + bias\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Matrix Operations\n",
    "\n",
    "- 위에서 곱과 합연산을 matrix operation을 활용하여 더욱 효율적으로 할 수 있습니다\n",
    "- syntax만 간단해지는것이 아니고, 컴퓨터 내부적으로 연산도 더욱 효율적으로 합니다\n",
    "\n",
    "[`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) 또는 [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) 를 활용합니다. `matmul()`은 broadcasting 가능한 함수입니다. \n",
    "\n",
    "1. 우선 다음관 같이 실행해보세요\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "- Python으로 코딩할때, 또는 NN 코딩을 할때 수없이 볼 수 있는 에러 메시지 입니다. \n",
    "- 여기 예제에서는 간단한 이유입니다\n",
    "- 위에서 `mm` 함수는 (내적, 또는 수학적 matrix 곱으로 정의됩니다) 즉,\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- 하지만, `features` and `weights` 모두 `(1, 5)` 차원이죠 (shape)\n",
    "- 결론적으로 `weights`의 차원을 바꿔줘야합니다\n",
    "- **Note** pytorch에서 data (features)의 차원은 row vector 입니다.\n",
    "\n",
    "**Note:** Tensor의 shape를 확인하기 위해서 `tensor.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping for Matrix multiplication \n",
    "\n",
    "\n",
    "차원 변경 복습:\n",
    "- [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), \n",
    "  - `weights.reshape(a, b)`는 `weights`와 같은 원소를 size `(a, b)`로 변경한 tensor를 리턴 합니다. 새로운 메모리에 저장합니다. (즉, `weights` tensor에는 변화가 없습니다.) \n",
    "- [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), \n",
    "  - `weights.resize_(a, b)` 위와 같은 작업을 합니다. 다만, 이경우 차원 변경하고자 하는 차원이 원래 원소를 다 포함하지 못하는 경우 절삭합니다. 더 많은 차원이면 초기화 없이 원소를 추가하여 만들어집니다. 언더바 `_`는 inplace operation을 수행하는 함수를 나타내는 약속입니다 (메모리 복사없이). 이 경우 직접 `weights`를 변경합니다.\n",
    "\n",
    "- [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "  - `weights`와 같은 원소를 size `(a, b)`로 변경한 tensor를 리턴 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.tensor([1,2,3,4])\n",
    "test.view(2,2)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.resize_(2,2)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 [2점]**: Exercise의 함수를 `torch.mm`을 사용하여 연산하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6619]])\n"
     ]
    }
   ],
   "source": [
    "## 답안작성\n",
    "\n",
    "y = torch.mm(features,weights.view(5,-1)) + bias\n",
    "\n",
    "\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks\n",
    "\n",
    "* 지금까지는 한개의 neuron이 있을 때를 가정하여 실습하였습니다\n",
    "* 이제 layer로 perceptron(neuron)을 쌓아서 동작하도록 해보겠습니다\n",
    "* 즉 각 perceptron의 출력은 다음 perceptron의 입력이 되는거죠\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "* 위 그림에서 첫 layer (가장아래) 는 inputs 즉, **input layer**\n",
    "* 중간 layer (아래에서 두번째) **hidden layer**\n",
    "* 그리고 가장위 출력을 담당하는 layer는 **output layer**라고 합니다\n",
    "* 위에서 살펴본 것 처럼, matrix operation을 통해서 각 layer에서 수행하는 연산을 할 수 있습니다. \n",
    "* 예를 들어서 중간 hidden layer $\\mathbf{h}=[h_1, h_2]$의 출력은 다음과 같이 연산합니다:\n",
    "$$\n",
    "\\mathbf{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* 각 layer 마다 출력을 다음 layer에 입력으로 받아서 연산을 수행하면 다음과 같습니다\n",
    "\n",
    "$$\n",
    "\\mathbf{y} =  f_2 \\! \\left(\\, f_1 \\! \\left(\\mathbf{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$\n",
    "\n",
    "* 위에서 $f_i()$ 함수들은 activation function으로 vector의 원소별로 연산됩니다 (위에서와 같이)\n",
    "* 아래 실습에서는 위에서 만든 activation function이 $f_1$, $f_2$라고 가정합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 [10점]:** 위에서 배운 2 layer network를 만들고 출력값을 확인하는 문제입니다. \n",
    "\n",
    "* 변수\n",
    "  * Network의 weight matrix `W1`(첫번째 layer) & `W2`(두번째 layer)\n",
    "  * Network의 bias vector, `B1` & `B2`\n",
    "  * input layer 크기(원소 수) n_input = 5\n",
    "  * hidden layer 크기(원소 수) n_input = 3\n",
    "  * output layer 크기(원소 수) n_input = 2\n",
    "* 생성\n",
    "  * (1, n_input) shape의 정규분포 data를 `features` 변수에 저장\n",
    "  * 위에서 정의한 크기에 맞추어서 `W_1`, `W_2` matrix를 정규분포로 생성\n",
    "  * 위에서 정의한 크기에 맞추어서 `B_1`, `B_2` vector를 정규분포로 생성\n",
    "* Output 연산\n",
    "  * hidden layer output `h`로 저장\n",
    "  * outer layer output `output`로 저장 (h가 입력 이겠죠)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features =  tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n",
      "W1 =  tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822],\n",
      "        [ 0.3177,  0.1328,  0.1373,  0.2405,  1.3955],\n",
      "        [ 1.3470,  2.4382,  0.2028,  2.4505,  2.0256]])\n",
      "W2 =  tensor([[ 1.7792, -0.9179, -0.4578],\n",
      "        [-0.7245,  1.2799, -0.9941]])\n",
      "h =  tensor([[0.9685, 0.8992, 2.6193]])\n",
      "tensor([[ 6.5940, -4.5667]])\n"
     ]
    }
   ],
   "source": [
    "### Data 생성 \n",
    "torch.manual_seed(7) # 답 확인을 위한 Seed 설정 \n",
    "\n",
    "# 네트워크 구조 size\n",
    "n_input =  5     # Number of input units\n",
    "n_hidden = 3     # Number of hidden units \n",
    "n_output = 2\n",
    "\n",
    "### 답안작성\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn(1,5)\n",
    "print('features = ',features)\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(3,5)\n",
    "print('W1 = ',W1)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(2,3)\n",
    "print('W2 = ',W2)\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn(1,3)\n",
    "B2 = torch.randn(1,2)\n",
    "\n",
    "h = torch.mm(features,W1.view(5,-1)) + B1\n",
    "print('h = ',h)\n",
    "\n",
    "\n",
    "output = torch.mm(h,W2.view(3,-1)) + B2\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘되었다면, 답이 `tensor([[0.9835, 0.0738]])`로 나왔을 것입니다\n",
    "\n",
    "Hidden unit의 크기는 보통  **hyperparameter** 의 한 종류로 간주합니다. 일반적으로 hidden layer의 수와 layer의 width가 클수록 성능이 좋아지는데, 반대로 overfitting, 연산복잡도 증가 등의 문제를 발생시킵니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
