{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Introduction to Deep Learning with Pytorch\n",
    "\n",
    "* 본 실습에서는 총 3개의 exercise가 있습니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "* 이번 실습에서는 [PyTorch](http://pytorch.org/)에 대해서 소개하겠니다. \n",
    "* numpy array 역시 tensor 이기때문에 Pytorch는 기본적으로 numpy와 매우 유사한 점이 많이 있습니다 (Pytorch의 디자인 철학이기도 합니다)\n",
    "* 또한 GPU를 활용하기 위해서 매우 편리한 방법을 제공합니다\n",
    "* 궁극적으로 network 만드는 것 부터 network training을 하기 위한 유용한 모듈을 모두 제공합니다. \n",
    "* 또한 Tensorflow에 비해서 Numpy/scipy와 더욱 유기적으로 작업이 가능합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "* Neural network (NN)을 활용하기 위해서 필요한 수학적 연산은 대부분 \n",
    "*tensors* 단위로 수행하는 선형 연산입니다\n",
    "* Tensor는 2차원 이상의 array이며 matrix, vector의 일반화된 개체입니다\n",
    " - vector 는 1-dimensional tensor\n",
    " - Matrix 는 2-dimensional tensor, \n",
    " - 3-dimensional tensor (예) RGB color images\n",
    "* 이와 같은 이유로 pytorch의 가장 기본적인 data structure는 tensors 입니다\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "그럼 첫 단계를 시작해보죠!\n",
    "\n",
    "1. python에서 필요한 package를 불러와야합니다\n",
    "2. pytorch 불러오는 명령어는 다음과 같습니다. 거의 항상 첫줄에 아래 명령어를 사용한다고 생각하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "# First, import PyTorch\n",
    "\n",
    "#torch 패키지를 불러온다.\n",
    "import torch\n",
    "import numpy as np\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tensor\n",
    "`torch.tensor()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "V_data = [1., 2., 3.]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "# Creates a matrix\n",
    "M_data = [[1., 2., 3.], [4., 5., 6]]\n",
    "M = torch.tensor(M_data)\n",
    "print(M)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "T_data = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing tensors\n",
    "* 1D tensor를 index로 들어 있는 자료를 불러오면 scalar (0D tensor)로 return\n",
    "  * python scalar로 불러오기위해서 item() 활용\n",
    "* 2D tensor를 index로 들어 있는 자료를 불러오면 vector (1D tensor)로 return\n",
    "* 3D tensor를 index로 들어 있는 자료를 불러오면 matrix (2D tensor)로 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor(2.)\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0])\n",
    "# Get a Python number from it\n",
    "print(V[0].item())# 값만 불러오기 위해 사용\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[0][1])\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dtype=torch.data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "V_integer = torch.tensor([1.0, 2.0, 3.0], dtype=torch.int)\n",
    "print(V_integer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate random Gaussians $\\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.7406, -1.0118,  0.4638, -0.3071,  0.1452],\n",
      "         [-1.3805,  0.5380,  1.6129, -0.1540, -1.4109],\n",
      "         [ 1.3949,  0.3928, -0.3801, -0.4246, -0.8367],\n",
      "         [ 0.2114, -1.0704, -0.7628,  2.0018,  1.2637]],\n",
      "\n",
      "        [[ 0.9124,  0.2211,  0.6594, -2.1463,  0.0903],\n",
      "         [ 0.0998,  0.4822, -0.9694, -0.5497,  0.1569],\n",
      "         [ 0.8536, -0.3293,  0.6815,  0.3578,  1.1874],\n",
      "         [ 0.8566,  0.5121,  0.5222,  1.9607,  0.3652]],\n",
      "\n",
      "        [[ 0.1299,  0.2500, -0.8542,  0.4376, -2.0960],\n",
      "         [ 1.7293,  0.7944, -1.8832,  0.0834,  0.4725],\n",
      "         [ 0.1065,  1.6914, -1.1708,  0.8595, -0.9490],\n",
      "         [-0.4279,  0.9945,  0.1833,  1.7559, -0.3789]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((3, 4, 5))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "y = torch.tensor([4., 5., 6.])\n",
    "z = x + y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2407,  0.8575, -0.0868,  1.6569, -0.1239],\n",
      "        [-0.4377,  0.7829,  0.2390, -0.2472,  0.6825],\n",
      "        [ 2.2316,  0.6601, -0.7915, -1.1909, -0.8113],\n",
      "        [ 1.2105, -0.3868, -0.5347, -0.4286, -0.4133],\n",
      "        [-0.3539,  0.7848,  0.2220, -1.9052,  1.7632]])\n",
      "tensor([[ 1.9673,  1.5108,  1.2928, -0.9184, -0.3289,  0.7116, -1.1284,  0.7913],\n",
      "        [-1.1322, -0.8267,  2.5374, -0.0592, -0.0226, -0.2185, -0.0826,  0.1348]])\n"
     ]
    }
   ],
   "source": [
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "# Tensor의 크기가 잘 맞지 않으면 error 발생함\n",
    "# torch.cat([x_1, x_2])\n",
    "# print(x_1.size())\n",
    "# print(x_2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Tensors\n",
    "* Use the .view() method to reshape a tensor\n",
    "* This method receives heavy use, because many neural network components expect their inputs to have a certain shape\n",
    "* Often you will need to reshape before passing your data to the component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0517,  0.2619, -0.5419,  0.6253],\n",
      "         [-0.4292,  0.4859,  0.0560, -0.7862],\n",
      "         [-0.4144,  0.7745,  0.1532,  0.2560]],\n",
      "\n",
      "        [[-0.9583,  0.1465,  0.1407, -1.7251],\n",
      "         [ 0.2827,  1.3205, -0.0999,  0.0183],\n",
      "         [-0.7159,  0.2780,  0.7799, -0.4495]]])\n",
      "tensor([[-1.0517,  0.2619, -0.5419,  0.6253, -0.4292,  0.4859,  0.0560, -0.7862,\n",
      "         -0.4144,  0.7745,  0.1532,  0.2560],\n",
      "        [-0.9583,  0.1465,  0.1407, -1.7251,  0.2827,  1.3205, -0.0999,  0.0183,\n",
      "         -0.7159,  0.2780,  0.7799, -0.4495]])\n",
      "tensor([[-1.0517,  0.2619, -0.5419,  0.6253, -0.4292,  0.4859,  0.0560, -0.7862,\n",
      "         -0.4144,  0.7745,  0.1532,  0.2560],\n",
      "        [-0.9583,  0.1465,  0.1407, -1.7251,  0.2827,  1.3205, -0.0999,  0.0183,\n",
      "         -0.7159,  0.2780,  0.7799, -0.4495]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "\n",
    "* 마무리 하면서, 간단하게 pytorch와 numpy간 자료변경하는 방법을 review 합니다\n",
    "* Numpy array에서 torch tensor로 자료변경은 `torch.from_numpy()` method를 사용\n",
    "* torch tensor를 numpy array로 변경은 `.numpy()` method를 사용합니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22738565, 0.11617435, 0.12148869],\n",
       "       [0.37713729, 0.57010557, 0.26147698],\n",
       "       [0.29290953, 0.52450962, 0.88551364],\n",
       "       [0.74267067, 0.3621317 , 0.80984931]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22738565, 0.11617435, 0.12148869],\n",
       "       [0.37713729, 0.57010557, 0.26147698],\n",
       "       [0.29290953, 0.52450962, 0.88551364],\n",
       "       [0.74267067, 0.3621317 , 0.80984931]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 위에서 수행한 모든 작업은 in-place 변환으로 (메모리를 새로 할당하지 않음) 두 개체는 memory 관점에서 같습니다. 즉, 하나를 변경하면 다른 하나도 변합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4548, 0.2323, 0.2430],\n",
       "        [0.7543, 1.1402, 0.5230],\n",
       "        [0.5858, 1.0490, 1.7710],\n",
       "        [1.4853, 0.7243, 1.6197]], dtype=torch.float64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4547713 , 0.2323487 , 0.24297737],\n",
       "       [0.75427459, 1.14021115, 0.52295396],\n",
       "       [0.58581907, 1.04901924, 1.77102728],\n",
       "       [1.48534135, 0.7242634 , 1.61969862]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "* Perceptron\n",
    " - Perceptron은 deep neural network에서 가장 기본적인 neuron을 본뜬 단위이며, 동작 방식은 input vector에 weight를 곱하고 더하여 (결국 innerproduct) activation function이라는 함수에 결과값을 출력하는 구조입니다. \n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "수식: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Vector inner product:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation function\n",
    "* 예제로 activation fuction 하나를 만들어 보겠습니다.\n",
    "* 다름 activation function을 완성하세요\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, feature, weight tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # random 함수사용을 위한 seed 설정입니다\n",
    "\n",
    "# Data (feature)\n",
    "features = torch.randn(1, 5)\n",
    "# True weights for our data, random normal variables\n",
    "weights = torch.randn_like(features)\n",
    "# bias term\n",
    "bias = torch.randn(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 각 줄별로 작업한 부분을 살펴보죠\n",
    "\n",
    " - `features = torch.randn((1, 5))` \n",
    "   - shape `(1, 5)` , one row and five columns, \n",
    "   - 정규분보 (평균 = 0, 표준편차 1)\n",
    "   - 왜 평균과 표준편차를 위와 같이 고정해도 되나요?\n",
    "\n",
    " - `weights = torch.randn_like(features)` \n",
    "   - `features`와 같은 shape로 randn을 불러옵니다 (편리하죠)\n",
    " -  `bias = torch.randn((1, 1))` \n",
    "   \n",
    "\n",
    "- PyTorch tensors 는 numpy arrray와 같이 기본적 연산이 가능합니다 (-,+,* 등) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 [2점]**: 위에서 정의한 `features`, `weights`, `bias`, `activation`을 활용하여 아래 수식을 와성하세요. \n",
    "  - [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum) 함수 또는, *some_tensor*`.sum()` 사용\n",
    "  - 아래 $f(\\cdot)$ 함수는 위에서 정의한  `activation` 함수입니다, $x$는 `feature`, $w$는 `weights`, $b$는 `bias`입니다\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "`Output`:\n",
    "\n",
    "`tensor([[0.1595]])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3734, 1.5328]])\n",
      "tensor(1.9061)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9061)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.randn(1,2)\n",
    "b=torch.randn(1,2)\n",
    "z=a+b\n",
    "print(z)\n",
    "print(z.sum())\n",
    "torch.sum(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "# 답 작성\n",
    "\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "y = activation((features * weights).sum() + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Matrix Operations\n",
    "\n",
    "- 위에서 곱과 합연산을 matrix operation을 활용하여 더욱 효율적으로 할 수 있습니다\n",
    "- syntax만 간단해지는것이 아니고, 컴퓨터 내부적으로 연산도 더욱 효율적으로 합니다\n",
    "\n",
    "[`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) 또는 [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul) 를 활용합니다. `matmul()`은 broadcasting 가능한 함수입니다. \n",
    "\n",
    "1. 우선 다음관 같이 실행해보세요\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "- Python으로 코딩할때, 또는 NN 코딩을 할때 수없이 볼 수 있는 에러 메시지 입니다. \n",
    "- 여기 예제에서는 간단한 이유입니다\n",
    "- 위에서 `mm` 함수는 (내적, 또는 수학적 matrix 곱으로 정의됩니다) 즉,\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- 하지만, `features` and `weights` 모두 `(1, 5)` 차원이죠 (shape)\n",
    "- 결론적으로 `weights`의 차원을 바꿔줘야합니다\n",
    "- **Note** pytorch에서 data (features)의 차원은 row vector 입니다.\n",
    "\n",
    "**Note:** Tensor의 shape를 확인하기 위해서 `tensor.shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [1 x 5], m2: [1 x 5] at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensorMath.cpp:136",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-eee0877cf81f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [1 x 5], m2: [1 x 5] at C:\\w\\1\\s\\windows\\pytorch\\aten\\src\\TH/generic/THTensorMath.cpp:136"
     ]
    }
   ],
   "source": [
    "torch.mm(features,weights)#torch.mm은 내적해주는거 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(features,weights.view(5,-1))+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias2=torch.tensor([1.])\n",
    "torch.mm(features,weights.view(5,-1))+bias2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping for Matrix multiplication \n",
    "\n",
    "\n",
    "차원 변경 복습:\n",
    "- [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), \n",
    "  - `weights.reshape(a, b)`는 `weights`와 같은 원소를 size `(a, b)`로 변경한 tensor를 리턴 합니다. 새로운 메모리에 저장합니다. (즉, `weights` tensor에는 변확가 없습니다.) \n",
    "- [`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), \n",
    "  - `weights.resize_(a, b)` 위와 같은 작업을 합니다. 다만, 이경우 차원 변경하고자 하는 차원이 원래 원소를 다 포함하지 못하는 경우 절삭합니다. 더 많은 차원이면 초기화 없이 원소를 추가하여 만들어집니다. 언더바 `_`는 inplace operation을 수행하는 함수를 나타내는 약속입니다 (메모리 복사없이). 이경우 직접 `weights`를 변경합니다.\n",
    "\n",
    "- [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
    "  - `weights`와 같은 원소를 size `(a, b)`로 변경한 tensor를 리턴 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.tensor([1,2,3,4])\n",
    "test.view(2,2)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.resize_(2,2)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 [2점]**: Exercise의 함수를 `torch.mm`을 사용하여 연산하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 답안작성\n",
    "\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks\n",
    "\n",
    "* 지금까지는 한개의 neuron이 있을 때를 가정하여 실습하였습니다\n",
    "* 이제 layer로 perceptron(neuron)을 쌓아서 동작하도록 해보겠습니다\n",
    "* 즉 각 perceptron의 출력은 다음 perceptron의 입력이 되는거죠\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "* 위 그림에서 첫 layer (가장아래) 는 inputs 즉, **input layer**\n",
    "* 중간 layer (아래에서 두번째) **hidden layer**\n",
    "* 그리고 가장위 출력을 담당하는 layer는 **output layer**라고 합니다\n",
    "* 위에서 살펴본 것 처럼, matrix operation을 통해서 각 layer에서 수행하는 연산을 할 수 있습니다. \n",
    "* 예를 들어서 중간 hidden layer $\\mathbf{h}=[h_1, h_2]$의 출력은 다음과 같이 연산합니다:\n",
    "$$\n",
    "\\mathbf{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* 각 layer 마다 출력을 다음 layer에 입력으로 받아서 연산을 수행하면 다음과 같습니다\n",
    "\n",
    "$$\n",
    "\\mathbf{y} =  f_2 \\! \\left(\\, f_1 \\! \\left(\\mathbf{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$\n",
    "\n",
    "* 위에서 $f_i()$ 함수들은 activation function으로 vector의 원소별로 연산됩니다 (위에서와 같이)\n",
    "* 아래 실습에서는 위에서 만든 activation function이 $f_1$, $f_2$라고 가정합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise [10점]:** 위에서 배운 2 layer network를 만들고 출력값을 확인하는 문제입니다. \n",
    "\n",
    "* 변수\n",
    "  * Network의 weight matrix `W1`(첫 layer) & `W2`(두번째 layer)\n",
    "  * Network의 bias vector, `B1` & `B2`\n",
    "  * input layer 크기(원소수) n_input = 5\n",
    "  * hidden layer 크기(원소수) n_input = 3\n",
    "  * output layer 크기(원소수) n_input = 2\n",
    "* 생성\n",
    "  * (1, n_input) shape의 정규분포 data를 `features` 변수에 저장\n",
    "  * 위에서 정의한 크기에 맞추어서 `W_1`, `W_2` matrix를 정규분포로 생성\n",
    "  * 위에서 정의한 크기에 맞추어서 `B_1`, `B_2` vector를 정규분포로 생성\n",
    "* Output 연산\n",
    "  * hidden layer output `h`로 저장\n",
    "  * outer layer output `output`로 저장 (h가 입력 이겠죠)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data 생성 \n",
    "torch.manual_seed(7) # 답 확인을 위한 Seed 설정 \n",
    "\n",
    "#fhidden layer와 width가 클수록 성능이 좋아지지만 overfitting (연산 복잡도 증가)문제를 일으킴\n",
    "# 네트워크 구조 size\n",
    "n_input =  5     # Number of input units\n",
    "n_hidden = 3                    # Number of hidden units \n",
    "n_output = 2\n",
    "\n",
    "\n",
    "\n",
    "### 답안작성\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, n_input))\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))\n",
    "\n",
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘되었다면, 답이 `tensor([[0.9835, 0.0738]])`로 나왔을 것입니다\n",
    "\n",
    "Hidden unit의 크기는 보통  **hyperparameter** 의 한 종류로 간주합니다. 일반적으로 hidden layer의 수와 layer의 width가 클수록 성능이 좋아지는데, 반대로 overfitting, 연산복잡도 증가 등의 문제를 발생시킵니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
